# -*- coding: utf-8 -*-
"""Assignment1_cs21m073.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l7RhVYH8RGnG3uATLZn1Thn7QqurRktn
"""

from sklearn.datasets import fetch_openml
from keras.utils.np_utils import to_categorical
import numpy as np
from sklearn.model_selection import train_test_split
import time
from keras.datasets import fashion_mnist
import keras
import matplotlib.pyplot as plt

!pip install wandb
import wandb

wandb.login()

sweep_config = {
    'method': 'grid', #grid, random
    'metric': {
      'name': 'acc',
      'goal': 'maximize'   
    },
    'parameters': {
        'epochs': {
            'values': [5, 10]
        },
        'batch_size': {
            'values': [16, 32 ,64]
        },
        
        'learning_rate': {
            'values': [1e-3, 1e-4]
        },
        
        'optimizer': {
            'values': ['sgd']
        },
    }
}

sweep_id = wandb.sweep(sweep_config, project="Assignment_1")

dataset = keras.datasets.fashion_mnist
(train_inputs, train_outputs), (test_inputs, tes_outputs) = dataset.load_data()

train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(train_inputs, train_outputs, test_size=0.15, random_state=24)

## Printing one image from each class
labels = ['T-shirt/top', 'Trouse', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
plt.figure(figsize=(10, 10))
flag = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
j=0
for i in range(130):
  plt.subplot(5, 5, j+1)
  plt.xticks([])
  plt.yticks([])
  if(flag[train_outputs[i]] == 0):
    plt.imshow(train_inputs[i], cmap=plt.cm.binary)
    plt.xlabel(labels[train_outputs[i]])
    flag[train_outputs[i]] = 1
    j = j+1

train_inputs = (train_inputs/255).astype('float32')
train_outputs = to_categorical(train_outputs)

val_inputs = (val_inputs/255).astype('float32')
val_outputs = to_categorical(val_outputs)

test_inputs = (test_inputs/255).astype('float32')
test_outputs = to_categorical(tes_outputs)

class FeedforwardNN():
    def __init__(self, sizes, layers, epochs, l_rate):
        self.sizes = sizes
        self.epochs = epochs
        self.l_rate = l_rate
        self.layers = layers
        # we save all parameters in the neural network in this dictionary
        (self.weights, self.bias) = self.initialization()
        self.a_values = dict()
        self.h_values = dict()

    def sigmoid(self, x):
        return 1/(1 + np.exp(-x))
    
    def dsigmoid(self, x):
      return (np.exp(-x))/((np.exp(-x)+1)**2)
    
    def ReLU(self, x):
      return np.maximum(0,x)

    def dReLU(self,x):
      return 1 * (x > 0) 

    def softmax(self, x, derivative=False):
        # Numerically stable with large exponentials
        exps = np.exp(x - x.max())
        if derivative:
            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))
        return exps / np.sum(exps, axis=0)

    def initialization(self):     
        weight = dict()
        bias = dict()
        for itr in range(1, self.layers):
           weight[itr] = np.random.randn(self.sizes[itr], self.sizes[itr-1]) * np.sqrt(1. / self.sizes[itr])
           bias[itr] = np.random.randn(self.sizes[itr], 1) * np.sqrt(1. / self.sizes[itr])
        return (weight, bias)


    def forward(self, x_train):

        # input layer activations becomes sample
        self.h_values[0] = x_train.reshape(-1)
        
        for itr in range(1, self.layers):
           self.a_values[itr] = np.add(np.dot(self.weights[itr], self.h_values[itr-1]), self.bias[itr].reshape(-1))
           if(itr == self.layers-1):
             self.h_values[itr] = self.softmax(self.a_values[itr])
           else:
             self.h_values[itr] = self.sigmoid(self.a_values[itr])

        return self.h_values[self.layers-1]

    def backward(self, y_train, output):
        delta_weight = dict()
        delta_bias = dict()

        #der_wrt_L = 2 * (output - y_train) / output.shape[0] * self.softmax(self.a_values[self.layers-1], derivative=True)
        der_wrt_L = -(y_train-output) * self.softmax(self.a_values[self.layers-1], derivative=True)
        for itr in range(self.layers-1, 0, -1):
          delta_weight[itr] = np.outer(der_wrt_L, self.h_values[itr-1])
          delta_bias[itr] = der_wrt_L
          if(itr > 1):
            der_wrt_prev_k = np.dot(self.weights[itr].T, der_wrt_L)
            der_wrt_L = der_wrt_prev_k * self.dsigmoid(self.a_values[itr-1])

        return (delta_weight, delta_bias)

    def update_parameters(self, changes_to_w, changes_to_b):        
        for key, value in changes_to_w.items():
            self.weights[key] -= self.l_rate * value
        
        for itr in range(1, self.layers):
          self.bias[itr] = np.subtract(self.bias[itr].reshape(-1) ,np.multiply(self.l_rate, changes_to_b[itr]))


    def compute_accuracy(self, x_val, y_val):
        predictions = []

        for x, y in zip(x_val, y_val):
            output = self.forward(x)
            pred = np.argmax(output)
            predictions.append(pred == np.argmax(y))
        
        return np.mean(predictions)

    def train(self, x_train, y_train, x_val, y_val):
          default_config={
          "learning_rate": self.l_rate,
          "epochs": self.epochs,
          "batch_size": 20,
          "optimizer":'sgd'
          }
          wandb.init(
          # Set the project where this run will be logged
          project="Assignment_1", 
          # Track hyperparameters and run metadata
          config=default_config)
          config = wandb.config
          start_time = time.time()        
          print("Training using mini batch Gradient Descent")
          
          j = 1
          delta_w = dict()
          delta_b = dict()
          temp = 0
          for iteration in range(config.epochs):
            j = 1
            for x, y in zip(x_train, y_train):
              y_hat = self.forward(x)
              (dw, db) = self.backward(y, y_hat)
              if(j == 1):
                  delta_w = dw
                  delta_b = db
                  j = j+1
              else:
                for itr in range(1, self.layers):
                  delta_w[itr] = np.add(delta_w[itr], dw[itr])
                  delta_b[itr] = np.add(delta_b[itr], db[itr])

              temp = temp + 1
              if(temp % config.batch_size == 0):
                self.update_parameters(delta_w,delta_b)
                delta_w = dict()
                delta_b = dict()
                temp = 0
                j = 1
            accuracy = self.compute_accuracy(x_val, y_val)     
            wandb.log({"acc": accuracy})       
            print('Epoch: {0}, Time Spent: {1:.2f}s, Val Accuracy: {2:.2f}%'.format(
                  iteration+1, time.time() - start_time, accuracy * 100
              ))
          wandb.finish()

def train():
  noOfLayers = 4
  sizes = [784, 128, 64, 10]
  learning_rate = 0.01
  epochs = 5
  batch_size = 20


  fnn = FeedforwardNN(sizes, noOfLayers, epochs, learning_rate)
  fnn.train(train_inputs, train_outputs, val_inputs, val_outputs)

wandb.agent(sweep_id, train)